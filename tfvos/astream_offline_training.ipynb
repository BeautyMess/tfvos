{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AStream` Offline Training\n",
    "This notebook performs offline training of the **appearance stream** on the **DAVIS 2016** dataset.\n",
    "\n",
    "MaskRNN's' binary segmentation net is a 2-stream convnet (`astream` and `fstream`):\n",
    "\n",
    "![](img/maskrnn.png)\n",
    "\n",
    "Section \"3.3 Binary Segmentation\" of the MaskRNN paper and \"Figure 2\" are inconsistent when it comes to describing the inputs of the two-stream network. In this implementation, we chose the input of the appearance stream `astream` to be the concatenation of the current frame I<sub>t</sub> and the warped prediction of the previous frame's segmentation mask b<sub>t-1</sub>, denoted as φ<sub>t-1,t</sub>(b<sub>t-1</sub>). The warping function φ<sub>t-1,t</sub>(.) transforms the input based on the optical flow fields from frame I<sub>t-1</sub> to frame I<sub>t</sub>. The `AStream` network takes 4-channel inputs (I<sub>t</sub>[0], I<sub>t</sub>[1], I<sub>t</sub>[2], φ<sub>t-1,t</sub>(b<sub>t-1</sub>)):\n",
    "\n",
    "The offline training of the `AStream` is done using a **4-channel input** VGG16 network pre-trained on ImageNet:\n",
    "\n",
    "![](img/osvos_parent.png)\n",
    "\n",
    "> Note: The VGG16 networks pretrained on Imagenet available online are **3-channel RGB input** models, so make sure you've run the [`VGG16 Surgery`](VGG16_Surgery.ipynb) notebook fist!\n",
    "\n",
    "To monitor training, run:\n",
    "```\n",
    "tensorboard --logdir E:\\repos\\tf-video-seg\\tfvos\\models\\astream_parent\n",
    "http://localhost:6006\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "astream_offline_training.ipynb\n",
    "\n",
    "AStream offline trainer\n",
    "\n",
    "Written by Phil Ferriere\n",
    "\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "\n",
    "Based on:\n",
    "  - https://github.com/scaelles/OSVOS-TensorFlow/blob/master/osvos_parent_demo.py\n",
    "    Written by Sergi Caelles (scaelles@vision.ee.ethz.ch)\n",
    "    This file is part of the OSVOS paper presented in:\n",
    "      Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, Luc Van Gool\n",
    "      One-Shot Video Object Segmentation\n",
    "      CVPR 2017\n",
    "    Unknown code license\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import model files\n",
    "import model\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model paths\n",
    "imagenet_ckpt = 'models/vgg_16_4chan.ckpt'\n",
    "segnet_stream = 'astream'\n",
    "ckpt_name= segnet_stream + '_parent'\n",
    "logs_path = os.path.join('models', ckpt_name)\n",
    "\n",
    "# Offline training parameters\n",
    "gpu_id = 0\n",
    "iter_mean_grad = 10\n",
    "max_training_iters_1 = 15000\n",
    "max_training_iters_2 = 30000\n",
    "max_training_iters_3 = 50000\n",
    "save_step = 5000\n",
    "test_image = None\n",
    "display_step = 100\n",
    "ini_learning_rate = 1e-8\n",
    "boundaries = [10000, 15000, 25000, 30000, 40000]\n",
    "values = [ini_learning_rate, ini_learning_rate * 0.1, ini_learning_rate, ini_learning_rate * 0.1, ini_learning_rate,\n",
    "          ini_learning_rate * 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "E:/datasets/davis2016/ImageSets/480p/train.txt\n",
      "Cache files:\n",
      "   videos container: E:/datasets/davis2016/davis2016_videos_train.npy\n",
      "   video_frame_idx container: E:/datasets/davis2016/davis2016_video_frame_idx_train.npy\n",
      "   images_train container: E:/datasets/davis2016/davis2016_images_train.npy\n",
      "   images_train_path container: E:/datasets/davis2016/davis2016_images_train_path.npy\n",
      "   masks_train container: E:/datasets/davis2016/davis2016_masks_train.npy\n",
      "   masks_train_path container: E:/datasets/davis2016/davis2016_masks_train_path.npy\n",
      "   flow_norms_train container: E:/datasets/davis2016/davis2016_flow_norms_train.npy\n",
      "   warped_prev_masks_train container: E:/datasets/davis2016/davis2016_warped_prev_masks_train.npy\n",
      "   masks_bboxes_train container: E:/datasets/davis2016/davis2016_masks_bboxes_train.npy\n",
      "Loading ndarrays from cache...\n",
      " videos container... done.\n",
      " video_frame_idx container... done.\n",
      " images_train container... done.\n",
      " images_train_path container... done.\n",
      " masks_train container... done.\n",
      " masks_train_path container... done.\n",
      " flow_norms_train container... done.\n",
      " warped_prev_masks_train container... done.\n",
      " masks_bboxes_train container... done.\n",
      "...done loading from cache.\n",
      "...done initializing Dataset\n"
     ]
    }
   ],
   "source": [
    "# Load DAVIS 2016 dataset\n",
    "options = datasets._DEFAULT_DAVIS16_OPTIONS\n",
    "# Set the following to True if you have a lot of RAM\n",
    "options['data_aug'] = False\n",
    "# Set the following to wherever you have downloaded the DAVIS 2016 dataset\n",
    "dataset_root = 'E:/datasets/davis2016/' if sys.platform.startswith(\"win\") else '/media/EDrive/datasets/davis2016/'\n",
    "train_file = dataset_root + 'ImageSets/480p/train.txt'\n",
    "dataset = datasets.davis16(train_file, None, dataset_root, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration:\n",
      "  in_memory            True\n",
      "  data_aug             False\n",
      "  use_cache            True\n",
      "  use_optical_flow     True\n",
      "  use_warped_masks     True\n",
      "  use_bboxes           True\n",
      "  optical_flow_mgr     pyflow\n"
     ]
    }
   ],
   "source": [
    "# Display dataset configuration\n",
    "dataset.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Layers:\n",
      "   name = astream/conv1/conv1_1/Relu:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/conv1/conv1_2/Relu:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/pool1/MaxPool:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/conv2/conv2_1/Relu:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/conv2/conv2_2/Relu:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/pool2/MaxPool:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/conv3/conv3_1/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv3/conv3_2/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv3/conv3_3/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/pool3/MaxPool:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv4/conv4_1/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv4/conv4_2/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv4/conv4_3/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/pool4/MaxPool:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_1/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_2/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_3/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv2_2_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv3_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv4_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv5_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-dsn_2/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_3/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_4/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_5/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_2-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_3-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_1:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_4-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_2:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_5-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_3:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-multi2-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_4:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi3-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_5:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi4-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_6:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi5-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_7:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/upscore-fuse/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "Network Parameters:\n",
      "   name = astream/conv1/conv1_1/weights:0, shape = (3, 3, 4, 64)\n",
      "   name = astream/conv1/conv1_1/biases:0, shape = (64,)\n",
      "   name = astream/conv1/conv1_2/weights:0, shape = (3, 3, 64, 64)\n",
      "   name = astream/conv1/conv1_2/biases:0, shape = (64,)\n",
      "   name = astream/conv2/conv2_1/weights:0, shape = (3, 3, 64, 128)\n",
      "   name = astream/conv2/conv2_1/biases:0, shape = (128,)\n",
      "   name = astream/conv2/conv2_2/weights:0, shape = (3, 3, 128, 128)\n",
      "   name = astream/conv2/conv2_2/biases:0, shape = (128,)\n",
      "   name = astream/conv3/conv3_1/weights:0, shape = (3, 3, 128, 256)\n",
      "   name = astream/conv3/conv3_1/biases:0, shape = (256,)\n",
      "   name = astream/conv3/conv3_2/weights:0, shape = (3, 3, 256, 256)\n",
      "   name = astream/conv3/conv3_2/biases:0, shape = (256,)\n",
      "   name = astream/conv3/conv3_3/weights:0, shape = (3, 3, 256, 256)\n",
      "   name = astream/conv3/conv3_3/biases:0, shape = (256,)\n",
      "   name = astream/conv4/conv4_1/weights:0, shape = (3, 3, 256, 512)\n",
      "   name = astream/conv4/conv4_1/biases:0, shape = (512,)\n",
      "   name = astream/conv4/conv4_2/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv4/conv4_2/biases:0, shape = (512,)\n",
      "   name = astream/conv4/conv4_3/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv4/conv4_3/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_1/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_1/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_2/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_2/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_3/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_3/biases:0, shape = (512,)\n",
      "   name = astream/conv2_2_16/weights:0, shape = (3, 3, 128, 16)\n",
      "   name = astream/conv2_2_16/biases:0, shape = (16,)\n",
      "   name = astream/conv3_3_16/weights:0, shape = (3, 3, 256, 16)\n",
      "   name = astream/conv3_3_16/biases:0, shape = (16,)\n",
      "   name = astream/conv4_3_16/weights:0, shape = (3, 3, 512, 16)\n",
      "   name = astream/conv4_3_16/biases:0, shape = (16,)\n",
      "   name = astream/conv5_3_16/weights:0, shape = (3, 3, 512, 16)\n",
      "   name = astream/conv5_3_16/biases:0, shape = (16,)\n",
      "   name = astream/score-dsn_2/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_2/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_3/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_3/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_4/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_4/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_5/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_5/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_2-up/weights:0, shape = (4, 4, 1, 1)\n",
      "   name = astream/score-dsn_3-up/weights:0, shape = (8, 8, 1, 1)\n",
      "   name = astream/score-dsn_4-up/weights:0, shape = (16, 16, 1, 1)\n",
      "   name = astream/score-dsn_5-up/weights:0, shape = (32, 32, 1, 1)\n",
      "   name = astream/score-multi2-up/weights:0, shape = (4, 4, 16, 16)\n",
      "   name = astream/score-multi3-up/weights:0, shape = (8, 8, 16, 16)\n",
      "   name = astream/score-multi4-up/weights:0, shape = (16, 16, 16, 16)\n",
      "   name = astream/score-multi5-up/weights:0, shape = (32, 32, 16, 16)\n",
      "   name = astream/upscore-fuse/weights:0, shape = (1, 1, 64, 1)\n",
      "   name = astream/upscore-fuse/biases:0, shape = (1,)\n",
      "Init variable\n",
      "Initializing from pre-trained imagenet model...\n",
      "INFO:tensorflow:Restoring parameters from models/vgg_16_4chan.ckpt\n",
      "Weights initialized\n",
      "Start training\n",
      "2018-01-30 23:59:18.335332 Iter 100: Training Loss = 27957.3730\n",
      "2018-01-31 00:03:14.152188 Iter 200: Training Loss = 14068.8477\n",
      "2018-01-31 00:07:10.485716 Iter 300: Training Loss = 2943.3499\n",
      "2018-01-31 00:11:06.521279 Iter 400: Training Loss = 33211.5000\n",
      "2018-01-31 00:15:02.546932 Iter 500: Training Loss = 19022.0312\n",
      "2018-01-31 00:18:58.488599 Iter 600: Training Loss = 7667.0156\n",
      "2018-01-31 00:22:54.271660 Iter 700: Training Loss = 7587.9233\n",
      "2018-01-31 00:26:49.686785 Iter 800: Training Loss = 10040.2314\n",
      "2018-01-31 00:30:44.241837 Iter 900: Training Loss = 23006.6562\n",
      "2018-01-31 00:34:40.307900 Iter 1000: Training Loss = 24995.8281\n",
      "2018-01-31 00:38:35.595221 Iter 1100: Training Loss = 24471.6445\n",
      "2018-01-31 00:42:31.142721 Iter 1200: Training Loss = 125885.5469\n",
      "2018-01-31 00:46:26.255859 Iter 1300: Training Loss = 2096.1003\n",
      "2018-01-31 00:50:21.259991 Iter 1400: Training Loss = 33199.3398\n",
      "2018-01-31 00:54:15.869647 Iter 1500: Training Loss = 20754.8340\n",
      "2018-01-31 00:58:10.483826 Iter 1600: Training Loss = 5332.3389\n",
      "2018-01-31 01:02:05.655361 Iter 1700: Training Loss = 10122.6465\n",
      "2018-01-31 01:05:59.849676 Iter 1800: Training Loss = 7297.2251\n",
      "2018-01-31 01:09:53.453654 Iter 1900: Training Loss = 6616.2114\n",
      "2018-01-31 01:13:46.810891 Iter 2000: Training Loss = 26552.2949\n",
      "2018-01-31 01:17:40.211414 Iter 2100: Training Loss = 26397.5371\n",
      "2018-01-31 01:21:34.522857 Iter 2200: Training Loss = 42124.2148\n",
      "2018-01-31 01:25:28.084645 Iter 2300: Training Loss = 20998.8887\n",
      "2018-01-31 01:29:21.359631 Iter 2400: Training Loss = 36141.8477\n",
      "2018-01-31 01:33:14.163523 Iter 2500: Training Loss = 20983.4609\n",
      "2018-01-31 01:37:06.740214 Iter 2600: Training Loss = 11439.4707\n",
      "2018-01-31 01:40:59.047970 Iter 2700: Training Loss = 8142.8506\n",
      "2018-01-31 01:44:51.693496 Iter 2800: Training Loss = 5306.8032\n",
      "2018-01-31 01:48:43.921610 Iter 2900: Training Loss = 12459.5898\n",
      "2018-01-31 01:52:36.547385 Iter 3000: Training Loss = 7418.3804\n",
      "2018-01-31 01:56:29.043528 Iter 3100: Training Loss = 18076.3438\n",
      "2018-01-31 02:00:23.864431 Iter 3200: Training Loss = 20252.7715\n",
      "2018-01-31 02:04:19.458128 Iter 3300: Training Loss = 13819.5205\n",
      "2018-01-31 02:08:14.689263 Iter 3400: Training Loss = 5333.2822\n",
      "2018-01-31 02:12:10.100612 Iter 3500: Training Loss = 62259.5195\n",
      "2018-01-31 02:16:05.464159 Iter 3600: Training Loss = 4610.2661\n",
      "2018-01-31 02:20:00.886671 Iter 3700: Training Loss = 27351.0039\n",
      "2018-01-31 02:23:55.999398 Iter 3800: Training Loss = 17105.0234\n",
      "2018-01-31 02:27:51.161270 Iter 3900: Training Loss = 6385.4395\n",
      "2018-01-31 02:31:45.745168 Iter 4000: Training Loss = 10238.2881\n",
      "2018-01-31 02:35:39.842058 Iter 4100: Training Loss = 16158.1123\n",
      "2018-01-31 02:39:34.246219 Iter 4200: Training Loss = 11305.3857\n",
      "2018-01-31 02:43:28.311947 Iter 4300: Training Loss = 17794.6680\n",
      "2018-01-31 02:47:22.455856 Iter 4400: Training Loss = 49113.5039\n",
      "2018-01-31 02:51:16.583593 Iter 4500: Training Loss = 1996.4320\n",
      "2018-01-31 02:55:10.369022 Iter 4600: Training Loss = 32191.0020\n",
      "2018-01-31 02:59:04.269226 Iter 4700: Training Loss = 57143.0664\n",
      "2018-01-31 03:02:58.016544 Iter 4800: Training Loss = 43502.3672\n",
      "2018-01-31 03:06:51.935341 Iter 4900: Training Loss = 3138.5024\n",
      "2018-01-31 03:10:46.014516 Iter 5000: Training Loss = 3146.0117\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-5000\n",
      "2018-01-31 03:14:43.688739 Iter 5100: Training Loss = 16753.0449\n",
      "2018-01-31 03:18:37.506767 Iter 5200: Training Loss = 3462.9631\n",
      "2018-01-31 03:22:31.482165 Iter 5300: Training Loss = 16508.2871\n",
      "2018-01-31 03:26:25.356455 Iter 5400: Training Loss = 29791.9199\n",
      "2018-01-31 03:30:19.204028 Iter 5500: Training Loss = 4641.8984\n",
      "2018-01-31 03:34:12.810002 Iter 5600: Training Loss = 10457.2402\n",
      "2018-01-31 03:38:06.291547 Iter 5700: Training Loss = 6286.0381\n",
      "2018-01-31 03:41:59.948949 Iter 5800: Training Loss = 5714.3115\n",
      "2018-01-31 03:45:53.624149 Iter 5900: Training Loss = 17500.6328\n",
      "2018-01-31 03:49:47.188439 Iter 6000: Training Loss = 23342.7266\n",
      "2018-01-31 03:53:40.538205 Iter 6100: Training Loss = 11713.4580\n",
      "2018-01-31 03:57:34.140702 Iter 6200: Training Loss = 6769.1514\n",
      "2018-01-31 04:01:27.553601 Iter 6300: Training Loss = 15862.7988\n",
      "2018-01-31 04:05:21.142989 Iter 6400: Training Loss = 9180.2217\n",
      "2018-01-31 04:09:14.614208 Iter 6500: Training Loss = 46602.1406\n",
      "2018-01-31 04:13:08.044229 Iter 6600: Training Loss = 68553.7031\n",
      "2018-01-31 04:17:01.225013 Iter 6700: Training Loss = 10737.6279\n",
      "2018-01-31 04:20:54.743746 Iter 6800: Training Loss = 3114.1465\n",
      "2018-01-31 04:24:48.120000 Iter 6900: Training Loss = 5501.6504\n",
      "2018-01-31 04:28:41.450161 Iter 7000: Training Loss = 10613.8506\n",
      "2018-01-31 04:32:34.870600 Iter 7100: Training Loss = 19376.6484\n",
      "2018-01-31 04:36:28.129136 Iter 7200: Training Loss = 18694.7617\n",
      "2018-01-31 04:40:21.449800 Iter 7300: Training Loss = 2928.6802\n",
      "2018-01-31 04:44:14.526930 Iter 7400: Training Loss = 31449.8047\n",
      "2018-01-31 04:48:07.843972 Iter 7500: Training Loss = 18090.9473\n",
      "2018-01-31 04:52:00.824797 Iter 7600: Training Loss = 10241.7295\n",
      "2018-01-31 04:55:53.885774 Iter 7700: Training Loss = 34856.6602\n",
      "2018-01-31 04:59:46.803861 Iter 7800: Training Loss = 8179.8345\n",
      "2018-01-31 05:03:40.008222 Iter 7900: Training Loss = 26046.5625\n",
      "2018-01-31 05:07:32.958521 Iter 8000: Training Loss = 11428.7559\n",
      "2018-01-31 05:11:25.880617 Iter 8100: Training Loss = 4731.1899\n",
      "2018-01-31 05:15:19.049368 Iter 8200: Training Loss = 12231.6797\n",
      "2018-01-31 05:19:11.896267 Iter 8300: Training Loss = 8541.1094\n",
      "2018-01-31 05:23:04.858469 Iter 8400: Training Loss = 5859.2393\n",
      "2018-01-31 05:26:57.985064 Iter 8500: Training Loss = 15796.7090\n",
      "2018-01-31 05:30:51.030989 Iter 8600: Training Loss = 10406.6621\n",
      "2018-01-31 05:34:44.058905 Iter 8700: Training Loss = 14120.8750\n",
      "2018-01-31 05:38:36.869203 Iter 8800: Training Loss = 35059.6172\n",
      "2018-01-31 05:42:29.473991 Iter 8900: Training Loss = 21174.0938\n",
      "2018-01-31 05:46:22.411701 Iter 9000: Training Loss = 27845.4434\n",
      "2018-01-31 05:50:15.317253 Iter 9100: Training Loss = 7800.8149\n",
      "2018-01-31 05:54:08.029792 Iter 9200: Training Loss = 12953.7969\n",
      "2018-01-31 05:58:00.872682 Iter 9300: Training Loss = 14562.0703\n",
      "2018-01-31 06:01:52.063496 Iter 9400: Training Loss = 24035.5410\n",
      "2018-01-31 06:05:41.935534 Iter 9500: Training Loss = 16931.3613\n",
      "2018-01-31 06:09:31.467561 Iter 9600: Training Loss = 11419.9287\n",
      "2018-01-31 06:13:21.133932 Iter 9700: Training Loss = 11877.7783\n",
      "2018-01-31 06:17:10.408930 Iter 9800: Training Loss = 11334.9619\n",
      "2018-01-31 06:20:59.834177 Iter 9900: Training Loss = 9627.1328\n",
      "2018-01-31 06:24:49.330894 Iter 10000: Training Loss = 7922.9082\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-10000\n",
      "2018-01-31 06:28:42.740088 Iter 10100: Training Loss = 15766.8047\n",
      "2018-01-31 06:32:32.379913 Iter 10200: Training Loss = 5895.6602\n",
      "2018-01-31 06:36:22.101676 Iter 10300: Training Loss = 15760.1963\n",
      "2018-01-31 06:40:11.818930 Iter 10400: Training Loss = 17970.6992\n",
      "2018-01-31 06:44:01.331699 Iter 10500: Training Loss = 12653.3916\n",
      "2018-01-31 06:47:51.152279 Iter 10600: Training Loss = 5333.0322\n",
      "2018-01-31 06:51:40.642462 Iter 10700: Training Loss = 7713.9673\n",
      "2018-01-31 06:55:30.452934 Iter 10800: Training Loss = 1752.9565\n",
      "2018-01-31 06:59:19.871836 Iter 10900: Training Loss = 6607.6069\n",
      "2018-01-31 07:03:09.101484 Iter 11000: Training Loss = 17969.2246\n",
      "2018-01-31 07:06:58.559758 Iter 11100: Training Loss = 11534.5088\n",
      "2018-01-31 07:10:48.148239 Iter 11200: Training Loss = 2821.5569\n",
      "2018-01-31 07:14:37.590099 Iter 11300: Training Loss = 8379.1934\n",
      "2018-01-31 07:18:27.113523 Iter 11400: Training Loss = 2677.9502\n",
      "2018-01-31 07:22:16.454130 Iter 11500: Training Loss = 15083.0576\n",
      "2018-01-31 07:26:05.954155 Iter 11600: Training Loss = 4759.9609\n",
      "2018-01-31 07:29:55.504557 Iter 11700: Training Loss = 4814.8486\n",
      "2018-01-31 07:33:44.936016 Iter 11800: Training Loss = 15718.4502\n",
      "2018-01-31 07:37:34.413424 Iter 11900: Training Loss = 3232.4519\n",
      "2018-01-31 07:41:24.034421 Iter 12000: Training Loss = 991.7189\n",
      "2018-01-31 07:45:13.581993 Iter 12100: Training Loss = 5299.4365\n",
      "2018-01-31 07:49:02.659963 Iter 12200: Training Loss = 7157.8149\n",
      "2018-01-31 07:52:51.983564 Iter 12300: Training Loss = 17110.7910\n",
      "2018-01-31 07:56:41.569444 Iter 12400: Training Loss = 2475.4949\n",
      "2018-01-31 08:00:31.082457 Iter 12500: Training Loss = 8619.4277\n",
      "2018-01-31 08:04:20.426122 Iter 12600: Training Loss = 5645.5195\n",
      "2018-01-31 08:08:10.188101 Iter 12700: Training Loss = 20772.9141\n",
      "2018-01-31 08:11:59.588370 Iter 12800: Training Loss = 14073.4590\n",
      "2018-01-31 08:15:48.975390 Iter 12900: Training Loss = 6631.6348\n",
      "2018-01-31 08:19:38.253211 Iter 13000: Training Loss = 4852.9902\n",
      "2018-01-31 08:23:27.810342 Iter 13100: Training Loss = 5633.2388\n",
      "2018-01-31 08:27:17.392655 Iter 13200: Training Loss = 2609.7070\n",
      "2018-01-31 08:31:08.222060 Iter 13300: Training Loss = 46191.6445\n",
      "2018-01-31 08:34:58.377462 Iter 13400: Training Loss = 11632.1729\n",
      "2018-01-31 08:38:48.355920 Iter 13500: Training Loss = 6554.2393\n",
      "2018-01-31 08:42:38.574897 Iter 13600: Training Loss = 23251.7500\n",
      "2018-01-31 08:46:28.547723 Iter 13700: Training Loss = 7877.7466\n",
      "2018-01-31 08:50:18.755826 Iter 13800: Training Loss = 1664.5052\n",
      "2018-01-31 08:54:08.740209 Iter 13900: Training Loss = 8757.5723\n",
      "2018-01-31 08:57:58.652344 Iter 14000: Training Loss = 2540.1389\n",
      "2018-01-31 09:01:48.517840 Iter 14100: Training Loss = 4290.2573\n",
      "2018-01-31 09:05:37.664114 Iter 14200: Training Loss = 14746.0869\n",
      "2018-01-31 09:09:26.728323 Iter 14300: Training Loss = 27814.4941\n",
      "2018-01-31 09:13:15.822718 Iter 14400: Training Loss = 3701.9419\n",
      "2018-01-31 09:17:05.035453 Iter 14500: Training Loss = 12497.7627\n",
      "2018-01-31 09:20:53.891163 Iter 14600: Training Loss = 15521.4678\n",
      "2018-01-31 09:24:45.157240 Iter 14700: Training Loss = 15041.4092\n",
      "2018-01-31 09:28:35.422224 Iter 14800: Training Loss = 6943.1206\n",
      "2018-01-31 09:32:25.574883 Iter 14900: Training Loss = 9262.4082\n",
      "2018-01-31 09:36:15.871206 Iter 15000: Training Loss = 46783.1445\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-15000\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "# Train the network with strong side outputs supervision\n",
    "with tf.Graph().as_default():\n",
    "    with tf.device('/gpu:' + str(gpu_id)):\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "        model.train_parent(dataset, imagenet_ckpt, 1, learning_rate, logs_path, max_training_iters_1, save_step,\n",
    "                           display_step, global_step, segnet_stream, iter_mean_grad=iter_mean_grad, test_image_path=test_image,\n",
    "                           ckpt_name=ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Layers:\n",
      "   name = astream/conv1/conv1_1/Relu:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/conv1/conv1_2/Relu:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/pool1/MaxPool:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/conv2/conv2_1/Relu:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/conv2/conv2_2/Relu:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/pool2/MaxPool:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/conv3/conv3_1/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv3/conv3_2/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv3/conv3_3/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/pool3/MaxPool:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv4/conv4_1/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv4/conv4_2/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv4/conv4_3/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/pool4/MaxPool:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_1/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_2/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_3/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv2_2_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv3_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv4_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv5_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-dsn_2/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_3/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_4/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_5/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_2-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_3-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_1:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_4-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_2:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_5-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_3:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-multi2-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_4:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi3-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_5:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi4-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_6:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi5-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_7:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/upscore-fuse/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "Network Parameters:\n",
      "   name = astream/conv1/conv1_1/weights:0, shape = (3, 3, 4, 64)\n",
      "   name = astream/conv1/conv1_1/biases:0, shape = (64,)\n",
      "   name = astream/conv1/conv1_2/weights:0, shape = (3, 3, 64, 64)\n",
      "   name = astream/conv1/conv1_2/biases:0, shape = (64,)\n",
      "   name = astream/conv2/conv2_1/weights:0, shape = (3, 3, 64, 128)\n",
      "   name = astream/conv2/conv2_1/biases:0, shape = (128,)\n",
      "   name = astream/conv2/conv2_2/weights:0, shape = (3, 3, 128, 128)\n",
      "   name = astream/conv2/conv2_2/biases:0, shape = (128,)\n",
      "   name = astream/conv3/conv3_1/weights:0, shape = (3, 3, 128, 256)\n",
      "   name = astream/conv3/conv3_1/biases:0, shape = (256,)\n",
      "   name = astream/conv3/conv3_2/weights:0, shape = (3, 3, 256, 256)\n",
      "   name = astream/conv3/conv3_2/biases:0, shape = (256,)\n",
      "   name = astream/conv3/conv3_3/weights:0, shape = (3, 3, 256, 256)\n",
      "   name = astream/conv3/conv3_3/biases:0, shape = (256,)\n",
      "   name = astream/conv4/conv4_1/weights:0, shape = (3, 3, 256, 512)\n",
      "   name = astream/conv4/conv4_1/biases:0, shape = (512,)\n",
      "   name = astream/conv4/conv4_2/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv4/conv4_2/biases:0, shape = (512,)\n",
      "   name = astream/conv4/conv4_3/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv4/conv4_3/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_1/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_1/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_2/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_2/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_3/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_3/biases:0, shape = (512,)\n",
      "   name = astream/conv2_2_16/weights:0, shape = (3, 3, 128, 16)\n",
      "   name = astream/conv2_2_16/biases:0, shape = (16,)\n",
      "   name = astream/conv3_3_16/weights:0, shape = (3, 3, 256, 16)\n",
      "   name = astream/conv3_3_16/biases:0, shape = (16,)\n",
      "   name = astream/conv4_3_16/weights:0, shape = (3, 3, 512, 16)\n",
      "   name = astream/conv4_3_16/biases:0, shape = (16,)\n",
      "   name = astream/conv5_3_16/weights:0, shape = (3, 3, 512, 16)\n",
      "   name = astream/conv5_3_16/biases:0, shape = (16,)\n",
      "   name = astream/score-dsn_2/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_2/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_3/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_3/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_4/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_4/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_5/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_5/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_2-up/weights:0, shape = (4, 4, 1, 1)\n",
      "   name = astream/score-dsn_3-up/weights:0, shape = (8, 8, 1, 1)\n",
      "   name = astream/score-dsn_4-up/weights:0, shape = (16, 16, 1, 1)\n",
      "   name = astream/score-dsn_5-up/weights:0, shape = (32, 32, 1, 1)\n",
      "   name = astream/score-multi2-up/weights:0, shape = (4, 4, 16, 16)\n",
      "   name = astream/score-multi3-up/weights:0, shape = (8, 8, 16, 16)\n",
      "   name = astream/score-multi4-up/weights:0, shape = (16, 16, 16, 16)\n",
      "   name = astream/score-multi5-up/weights:0, shape = (32, 32, 16, 16)\n",
      "   name = astream/upscore-fuse/weights:0, shape = (1, 1, 64, 1)\n",
      "   name = astream/upscore-fuse/biases:0, shape = (1,)\n",
      "Init variable\n",
      "Initializing from previous checkpoint...\n",
      "INFO:tensorflow:Restoring parameters from models\\astream_parent\\astream_parent.ckpt-15000\n",
      "Weights initialized\n",
      "Start training\n",
      "2018-01-31 09:40:20.044676 Iter 15100: Training Loss = 12934.5908\n",
      "2018-01-31 09:44:10.571284 Iter 15200: Training Loss = 2593.1455\n",
      "2018-01-31 09:48:00.848310 Iter 15300: Training Loss = 8213.2178\n",
      "2018-01-31 09:51:50.940664 Iter 15400: Training Loss = 827.4257\n",
      "2018-01-31 09:55:41.020354 Iter 15500: Training Loss = 14303.2354\n",
      "2018-01-31 09:59:29.976736 Iter 15600: Training Loss = 1096.9552\n",
      "2018-01-31 10:03:19.058557 Iter 15700: Training Loss = 5547.6177\n",
      "2018-01-31 10:07:08.218901 Iter 15800: Training Loss = 6519.4990\n",
      "2018-01-31 10:10:59.248839 Iter 15900: Training Loss = 2021.3441\n",
      "2018-01-31 10:14:50.776537 Iter 16000: Training Loss = 6512.1948\n",
      "2018-01-31 10:18:41.405575 Iter 16100: Training Loss = 929.1011\n",
      "2018-01-31 10:22:32.311971 Iter 16200: Training Loss = 3572.4749\n",
      "2018-01-31 10:26:23.171553 Iter 16300: Training Loss = 8767.9229\n",
      "2018-01-31 10:30:14.110250 Iter 16400: Training Loss = 8907.5479\n",
      "2018-01-31 10:34:04.662927 Iter 16500: Training Loss = 8822.0908\n",
      "2018-01-31 10:37:55.282789 Iter 16600: Training Loss = 1038.7520\n",
      "2018-01-31 10:41:45.999099 Iter 16700: Training Loss = 3012.7773\n",
      "2018-01-31 10:45:36.904831 Iter 16800: Training Loss = 5317.7085\n",
      "2018-01-31 10:49:27.464071 Iter 16900: Training Loss = 2563.9373\n",
      "2018-01-31 10:53:18.526202 Iter 17000: Training Loss = 12743.9404\n",
      "2018-01-31 10:57:09.496766 Iter 17100: Training Loss = 8771.4141\n",
      "2018-01-31 11:01:00.366226 Iter 17200: Training Loss = 5018.7139\n",
      "2018-01-31 11:04:51.576857 Iter 17300: Training Loss = 0.4076\n",
      "2018-01-31 11:08:42.505962 Iter 17400: Training Loss = 7463.6655\n",
      "2018-01-31 11:12:33.389814 Iter 17500: Training Loss = 6347.6943\n",
      "2018-01-31 11:16:24.578807 Iter 17600: Training Loss = 3659.5974\n",
      "2018-01-31 11:20:15.681533 Iter 17700: Training Loss = 8962.2227\n",
      "2018-01-31 11:24:06.761912 Iter 17800: Training Loss = 36798.0391\n",
      "2018-01-31 11:27:57.578520 Iter 17900: Training Loss = 1813.6644\n",
      "2018-01-31 11:31:48.539046 Iter 18000: Training Loss = 7856.9951\n",
      "2018-01-31 11:35:38.996875 Iter 18100: Training Loss = 4248.5757\n",
      "2018-01-31 11:39:28.271053 Iter 18200: Training Loss = 8559.8721\n",
      "2018-01-31 11:43:17.664494 Iter 18300: Training Loss = 1238.8456\n",
      "2018-01-31 11:47:07.101946 Iter 18400: Training Loss = 5539.0312\n",
      "2018-01-31 11:50:56.745301 Iter 18500: Training Loss = 7741.6011\n",
      "2018-01-31 11:54:46.198311 Iter 18600: Training Loss = 12726.2295\n",
      "2018-01-31 11:58:35.570787 Iter 18700: Training Loss = 13527.7998\n",
      "2018-01-31 12:02:24.928726 Iter 18800: Training Loss = 6244.1211\n",
      "2018-01-31 12:06:14.603983 Iter 18900: Training Loss = 3439.1709\n",
      "2018-01-31 12:10:04.104392 Iter 19000: Training Loss = 9096.7549\n",
      "2018-01-31 12:13:53.680472 Iter 19100: Training Loss = 941.1245\n",
      "2018-01-31 12:17:43.480355 Iter 19200: Training Loss = 10253.3223\n",
      "2018-01-31 12:21:32.897021 Iter 19300: Training Loss = 3462.3376\n",
      "2018-01-31 12:25:22.436879 Iter 19400: Training Loss = 4856.3276\n",
      "2018-01-31 12:29:12.006600 Iter 19500: Training Loss = 2742.6792\n",
      "2018-01-31 12:33:03.782197 Iter 19600: Training Loss = 2028.5703\n",
      "2018-01-31 12:36:54.901502 Iter 19700: Training Loss = 21902.3281\n",
      "2018-01-31 12:40:45.793558 Iter 19800: Training Loss = 2683.3865\n",
      "2018-01-31 12:44:36.764242 Iter 19900: Training Loss = 8556.7266\n",
      "2018-01-31 12:48:28.067382 Iter 20000: Training Loss = 5962.1523\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-20000\n",
      "2018-01-31 12:52:22.736811 Iter 20100: Training Loss = 5069.5044\n",
      "2018-01-31 12:56:13.843465 Iter 20200: Training Loss = 8656.3838\n",
      "2018-01-31 13:00:04.591388 Iter 20300: Training Loss = 5150.4976\n",
      "2018-01-31 13:03:55.609638 Iter 20400: Training Loss = 8228.6250\n",
      "2018-01-31 13:07:46.113411 Iter 20500: Training Loss = 26945.7930\n",
      "2018-01-31 13:11:36.805021 Iter 20600: Training Loss = 5054.5688\n",
      "2018-01-31 13:15:27.489020 Iter 20700: Training Loss = 5131.1963\n",
      "2018-01-31 13:19:18.673762 Iter 20800: Training Loss = 11735.2998\n",
      "2018-01-31 13:23:08.392862 Iter 20900: Training Loss = 3354.5457\n",
      "2018-01-31 13:26:57.713182 Iter 21000: Training Loss = 1682.3441\n",
      "2018-01-31 13:30:47.203147 Iter 21100: Training Loss = 5168.5229\n",
      "2018-01-31 13:34:36.492442 Iter 21200: Training Loss = 6556.3501\n",
      "2018-01-31 13:38:25.875869 Iter 21300: Training Loss = 5966.7964\n",
      "2018-01-31 13:42:15.586697 Iter 21400: Training Loss = 7707.9790\n",
      "2018-01-31 13:46:05.383202 Iter 21500: Training Loss = 21818.1270\n",
      "2018-01-31 13:49:55.161769 Iter 21600: Training Loss = 8623.8389\n",
      "2018-01-31 13:53:44.732184 Iter 21700: Training Loss = 8221.9707\n",
      "2018-01-31 13:57:34.204297 Iter 21800: Training Loss = 2447.9668\n",
      "2018-01-31 14:01:23.931646 Iter 21900: Training Loss = 33349.1445\n",
      "2018-01-31 14:05:13.517941 Iter 22000: Training Loss = 19073.9648\n",
      "2018-01-31 14:09:03.072541 Iter 22100: Training Loss = 4567.3760\n",
      "2018-01-31 14:12:52.793901 Iter 22200: Training Loss = 9422.2246\n",
      "2018-01-31 14:16:42.417973 Iter 22300: Training Loss = 6896.9839\n",
      "2018-01-31 14:20:31.931426 Iter 22400: Training Loss = 0.4077\n",
      "2018-01-31 14:24:21.599815 Iter 22500: Training Loss = 1807.7424\n",
      "2018-01-31 14:28:11.445130 Iter 22600: Training Loss = 3308.5012\n",
      "2018-01-31 14:32:00.902491 Iter 22700: Training Loss = 2390.1704\n",
      "2018-01-31 14:35:50.481068 Iter 22800: Training Loss = 9546.8057\n",
      "2018-01-31 14:39:40.008763 Iter 22900: Training Loss = 1977.1360\n",
      "2018-01-31 14:43:29.558666 Iter 23000: Training Loss = 629.1034\n",
      "2018-01-31 14:47:19.352736 Iter 23100: Training Loss = 4955.9780\n",
      "2018-01-31 14:51:08.933450 Iter 23200: Training Loss = 1626.4464\n",
      "2018-01-31 14:54:58.276051 Iter 23300: Training Loss = 3202.0303\n",
      "2018-01-31 14:58:47.662647 Iter 23400: Training Loss = 4694.1338\n",
      "2018-01-31 15:02:36.746661 Iter 23500: Training Loss = 4872.0171\n",
      "2018-01-31 15:06:27.704647 Iter 23600: Training Loss = 11193.7246\n",
      "2018-01-31 15:10:19.810353 Iter 23700: Training Loss = 1750.5598\n",
      "2018-01-31 15:14:11.765719 Iter 23800: Training Loss = 8265.9307\n",
      "2018-01-31 15:18:02.662771 Iter 23900: Training Loss = 17525.1035\n",
      "2018-01-31 15:21:53.721442 Iter 24000: Training Loss = 1841.5920\n",
      "2018-01-31 15:25:44.822260 Iter 24100: Training Loss = 57497.1875\n",
      "2018-01-31 15:29:35.878126 Iter 24200: Training Loss = 9307.3574\n",
      "2018-01-31 15:33:26.683667 Iter 24300: Training Loss = 8092.6382\n",
      "2018-01-31 15:37:17.578278 Iter 24400: Training Loss = 7982.6665\n",
      "2018-01-31 15:41:08.488149 Iter 24500: Training Loss = 4333.7764\n",
      "2018-01-31 15:45:00.031909 Iter 24600: Training Loss = 11802.4004\n",
      "2018-01-31 15:48:50.866328 Iter 24700: Training Loss = 4677.1279\n",
      "2018-01-31 15:52:41.881985 Iter 24800: Training Loss = 4814.3228\n",
      "2018-01-31 15:56:33.181753 Iter 24900: Training Loss = 1023.4664\n",
      "2018-01-31 16:00:24.185313 Iter 25000: Training Loss = 12069.4531\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-25000\n",
      "2018-01-31 16:04:18.637069 Iter 25100: Training Loss = 22320.3438\n",
      "2018-01-31 16:08:09.541738 Iter 25200: Training Loss = 3561.0720\n",
      "2018-01-31 16:12:00.446074 Iter 25300: Training Loss = 7436.7686\n",
      "2018-01-31 16:15:51.220605 Iter 25400: Training Loss = 7656.1133\n",
      "2018-01-31 16:19:42.960123 Iter 25500: Training Loss = 3220.8359\n",
      "2018-01-31 16:23:34.040279 Iter 25600: Training Loss = 543.6337\n",
      "2018-01-31 16:27:25.101047 Iter 25700: Training Loss = 7223.2266\n",
      "2018-01-31 16:31:16.160231 Iter 25800: Training Loss = 11691.9609\n",
      "2018-01-31 16:35:07.020715 Iter 25900: Training Loss = 8839.8213\n",
      "2018-01-31 16:38:57.871060 Iter 26000: Training Loss = 9951.1924\n",
      "2018-01-31 16:42:48.955135 Iter 26100: Training Loss = 1786.2635\n",
      "2018-01-31 16:46:39.785645 Iter 26200: Training Loss = 3486.4417\n",
      "2018-01-31 16:50:29.921601 Iter 26300: Training Loss = 3348.9788\n",
      "2018-01-31 16:54:19.495487 Iter 26400: Training Loss = 12782.7441\n",
      "2018-01-31 16:58:09.065119 Iter 26500: Training Loss = 7100.3496\n",
      "2018-01-31 17:01:58.648343 Iter 26600: Training Loss = 5131.9927\n",
      "2018-01-31 17:05:48.275442 Iter 26700: Training Loss = 9896.7930\n",
      "2018-01-31 17:09:37.786824 Iter 26800: Training Loss = 4028.3833\n",
      "2018-01-31 17:13:29.322875 Iter 26900: Training Loss = 7207.6870\n",
      "2018-01-31 17:17:20.185907 Iter 27000: Training Loss = 222.8595\n",
      "2018-01-31 17:21:11.066987 Iter 27100: Training Loss = 6096.9233\n",
      "2018-01-31 17:25:01.817851 Iter 27200: Training Loss = 12068.8887\n",
      "2018-01-31 17:28:52.494722 Iter 27300: Training Loss = 2573.3833\n",
      "2018-01-31 17:32:43.425007 Iter 27400: Training Loss = 1471.8706\n",
      "2018-01-31 17:36:33.879112 Iter 27500: Training Loss = 20003.5020\n",
      "2018-01-31 17:40:24.382166 Iter 27600: Training Loss = 3658.0859\n",
      "2018-01-31 17:44:13.642642 Iter 27700: Training Loss = 3527.9407\n",
      "2018-01-31 17:48:02.785134 Iter 27800: Training Loss = 14482.0605\n",
      "2018-01-31 17:51:51.540955 Iter 27900: Training Loss = 8668.7598\n",
      "2018-01-31 17:55:40.419763 Iter 28000: Training Loss = 6008.4629\n",
      "2018-01-31 17:59:29.440627 Iter 28100: Training Loss = 5233.4756\n",
      "2018-01-31 18:03:18.168985 Iter 28200: Training Loss = 5147.8247\n",
      "2018-01-31 18:07:07.057579 Iter 28300: Training Loss = 7659.4858\n",
      "2018-01-31 18:10:56.013926 Iter 28400: Training Loss = 4788.6240\n",
      "2018-01-31 18:14:44.678170 Iter 28500: Training Loss = 3999.0034\n",
      "2018-01-31 18:18:33.342942 Iter 28600: Training Loss = 3780.6077\n",
      "2018-01-31 18:22:22.101969 Iter 28700: Training Loss = 2448.6123\n",
      "2018-01-31 18:26:10.834748 Iter 28800: Training Loss = 7529.0947\n",
      "2018-01-31 18:29:59.464139 Iter 28900: Training Loss = 5566.2422\n",
      "2018-01-31 18:33:47.897967 Iter 29000: Training Loss = 9067.4785\n",
      "2018-01-31 18:37:36.214265 Iter 29100: Training Loss = 8026.9507\n",
      "2018-01-31 18:41:24.756610 Iter 29200: Training Loss = 2276.1018\n",
      "2018-01-31 18:45:13.624975 Iter 29300: Training Loss = 7202.7329\n",
      "2018-01-31 18:49:02.267855 Iter 29400: Training Loss = 2117.0857\n",
      "2018-01-31 18:52:50.864488 Iter 29500: Training Loss = 3525.7793\n",
      "2018-01-31 18:56:36.869799 Iter 29600: Training Loss = 4866.6045\n",
      "2018-01-31 19:00:25.331929 Iter 29700: Training Loss = 5418.3037\n",
      "2018-01-31 19:04:13.833527 Iter 29800: Training Loss = 1738.2188\n",
      "2018-01-31 19:08:02.395627 Iter 29900: Training Loss = 21776.8086\n",
      "2018-01-31 19:11:50.919341 Iter 30000: Training Loss = 9114.9023\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-30000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "# Train the network with weak side outputs supervision\n",
    "with tf.Graph().as_default():\n",
    "    with tf.device('/gpu:' + str(gpu_id)):\n",
    "        global_step = tf.Variable(max_training_iters_1, name='global_step', trainable=False)\n",
    "        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "        model.train_parent(dataset, imagenet_ckpt, 2, learning_rate, logs_path, max_training_iters_2, save_step,\n",
    "                           display_step, global_step, segnet_stream, iter_mean_grad=iter_mean_grad, resume_training=True,\n",
    "                           test_image_path=test_image, ckpt_name=ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Layers:\n",
      "   name = astream/conv1/conv1_1/Relu:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/conv1/conv1_2/Relu:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/pool1/MaxPool:0, shape = (1, ?, ?, 64)\n",
      "   name = astream/conv2/conv2_1/Relu:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/conv2/conv2_2/Relu:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/pool2/MaxPool:0, shape = (1, ?, ?, 128)\n",
      "   name = astream/conv3/conv3_1/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv3/conv3_2/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv3/conv3_3/Relu:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/pool3/MaxPool:0, shape = (1, ?, ?, 256)\n",
      "   name = astream/conv4/conv4_1/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv4/conv4_2/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv4/conv4_3/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/pool4/MaxPool:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_1/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_2/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv5/conv5_3/Relu:0, shape = (1, ?, ?, 512)\n",
      "   name = astream/conv2_2_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv3_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv4_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/conv5_3_16/BiasAdd:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-dsn_2/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_3/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_4/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_5/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_2-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_3-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_1:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_4-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_2:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-dsn_5-up/conv2d_transpose:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/Reshape_3:0, shape = (1, ?, ?, 1)\n",
      "   name = astream/score-multi2-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_4:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi3-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_5:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi4-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_6:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/score-multi5-up/conv2d_transpose:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/Reshape_7:0, shape = (1, ?, ?, 16)\n",
      "   name = astream/upscore-fuse/BiasAdd:0, shape = (1, ?, ?, 1)\n",
      "Network Parameters:\n",
      "   name = astream/conv1/conv1_1/weights:0, shape = (3, 3, 4, 64)\n",
      "   name = astream/conv1/conv1_1/biases:0, shape = (64,)\n",
      "   name = astream/conv1/conv1_2/weights:0, shape = (3, 3, 64, 64)\n",
      "   name = astream/conv1/conv1_2/biases:0, shape = (64,)\n",
      "   name = astream/conv2/conv2_1/weights:0, shape = (3, 3, 64, 128)\n",
      "   name = astream/conv2/conv2_1/biases:0, shape = (128,)\n",
      "   name = astream/conv2/conv2_2/weights:0, shape = (3, 3, 128, 128)\n",
      "   name = astream/conv2/conv2_2/biases:0, shape = (128,)\n",
      "   name = astream/conv3/conv3_1/weights:0, shape = (3, 3, 128, 256)\n",
      "   name = astream/conv3/conv3_1/biases:0, shape = (256,)\n",
      "   name = astream/conv3/conv3_2/weights:0, shape = (3, 3, 256, 256)\n",
      "   name = astream/conv3/conv3_2/biases:0, shape = (256,)\n",
      "   name = astream/conv3/conv3_3/weights:0, shape = (3, 3, 256, 256)\n",
      "   name = astream/conv3/conv3_3/biases:0, shape = (256,)\n",
      "   name = astream/conv4/conv4_1/weights:0, shape = (3, 3, 256, 512)\n",
      "   name = astream/conv4/conv4_1/biases:0, shape = (512,)\n",
      "   name = astream/conv4/conv4_2/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv4/conv4_2/biases:0, shape = (512,)\n",
      "   name = astream/conv4/conv4_3/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv4/conv4_3/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_1/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_1/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_2/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_2/biases:0, shape = (512,)\n",
      "   name = astream/conv5/conv5_3/weights:0, shape = (3, 3, 512, 512)\n",
      "   name = astream/conv5/conv5_3/biases:0, shape = (512,)\n",
      "   name = astream/conv2_2_16/weights:0, shape = (3, 3, 128, 16)\n",
      "   name = astream/conv2_2_16/biases:0, shape = (16,)\n",
      "   name = astream/conv3_3_16/weights:0, shape = (3, 3, 256, 16)\n",
      "   name = astream/conv3_3_16/biases:0, shape = (16,)\n",
      "   name = astream/conv4_3_16/weights:0, shape = (3, 3, 512, 16)\n",
      "   name = astream/conv4_3_16/biases:0, shape = (16,)\n",
      "   name = astream/conv5_3_16/weights:0, shape = (3, 3, 512, 16)\n",
      "   name = astream/conv5_3_16/biases:0, shape = (16,)\n",
      "   name = astream/score-dsn_2/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_2/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_3/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_3/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_4/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_4/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_5/weights:0, shape = (1, 1, 16, 1)\n",
      "   name = astream/score-dsn_5/biases:0, shape = (1,)\n",
      "   name = astream/score-dsn_2-up/weights:0, shape = (4, 4, 1, 1)\n",
      "   name = astream/score-dsn_3-up/weights:0, shape = (8, 8, 1, 1)\n",
      "   name = astream/score-dsn_4-up/weights:0, shape = (16, 16, 1, 1)\n",
      "   name = astream/score-dsn_5-up/weights:0, shape = (32, 32, 1, 1)\n",
      "   name = astream/score-multi2-up/weights:0, shape = (4, 4, 16, 16)\n",
      "   name = astream/score-multi3-up/weights:0, shape = (8, 8, 16, 16)\n",
      "   name = astream/score-multi4-up/weights:0, shape = (16, 16, 16, 16)\n",
      "   name = astream/score-multi5-up/weights:0, shape = (32, 32, 16, 16)\n",
      "   name = astream/upscore-fuse/weights:0, shape = (1, 1, 64, 1)\n",
      "   name = astream/upscore-fuse/biases:0, shape = (1,)\n",
      "Init variable\n",
      "Initializing from previous checkpoint...\n",
      "INFO:tensorflow:Restoring parameters from models\\astream_parent\\astream_parent.ckpt-30000\n",
      "Weights initialized\n",
      "Start training\n",
      "2018-01-31 19:15:39.877342 Iter 30100: Training Loss = 960.6976\n",
      "2018-01-31 19:19:17.867448 Iter 30200: Training Loss = 2471.3872\n",
      "2018-01-31 19:22:55.840132 Iter 30300: Training Loss = 910.4039\n",
      "2018-01-31 19:26:33.443081 Iter 30400: Training Loss = 2287.9954\n",
      "2018-01-31 19:30:11.329227 Iter 30500: Training Loss = 699.5270\n",
      "2018-01-31 19:33:49.099946 Iter 30600: Training Loss = 258.6932\n",
      "2018-01-31 19:37:26.942422 Iter 30700: Training Loss = 920.6009\n",
      "2018-01-31 19:41:04.890971 Iter 30800: Training Loss = 1350.8658\n",
      "2018-01-31 19:44:42.985900 Iter 30900: Training Loss = 1008.7401\n",
      "2018-01-31 19:48:20.681793 Iter 31000: Training Loss = 1700.8828\n",
      "2018-01-31 19:51:58.494410 Iter 31100: Training Loss = 1681.8855\n",
      "2018-01-31 19:55:36.402967 Iter 31200: Training Loss = 1348.6287\n",
      "2018-01-31 19:59:14.299201 Iter 31300: Training Loss = 425.8920\n",
      "2018-01-31 20:02:52.143842 Iter 31400: Training Loss = 523.3002\n",
      "2018-01-31 20:06:29.785269 Iter 31500: Training Loss = 290.7360\n",
      "2018-01-31 20:10:07.592736 Iter 31600: Training Loss = 350.3544\n",
      "2018-01-31 20:13:45.380211 Iter 31700: Training Loss = 520.0344\n",
      "2018-01-31 20:17:22.951245 Iter 31800: Training Loss = 2481.6150\n",
      "2018-01-31 20:21:00.405585 Iter 31900: Training Loss = 1647.0551\n",
      "2018-01-31 20:24:37.860158 Iter 32000: Training Loss = 1128.7717\n",
      "2018-01-31 20:28:15.540543 Iter 32100: Training Loss = 1056.3696\n",
      "2018-01-31 20:31:53.202661 Iter 32200: Training Loss = 2176.6860\n",
      "2018-01-31 20:35:30.891775 Iter 32300: Training Loss = 404.1539\n",
      "2018-01-31 20:39:08.367801 Iter 32400: Training Loss = 958.6183\n",
      "2018-01-31 20:42:45.989598 Iter 32500: Training Loss = 489.6363\n",
      "2018-01-31 20:46:23.468452 Iter 32600: Training Loss = 1146.4935\n",
      "2018-01-31 20:50:01.167803 Iter 32700: Training Loss = 1127.2493\n",
      "2018-01-31 20:53:39.479686 Iter 32800: Training Loss = 417.0170\n",
      "2018-01-31 20:57:17.060037 Iter 32900: Training Loss = 1593.7543\n",
      "2018-01-31 21:00:54.534053 Iter 33000: Training Loss = 1494.1744\n",
      "2018-01-31 21:04:31.648842 Iter 33100: Training Loss = 170.8358\n",
      "2018-01-31 21:08:08.912624 Iter 33200: Training Loss = 220.6855\n",
      "2018-01-31 21:11:46.382928 Iter 33300: Training Loss = 635.7209\n",
      "2018-01-31 21:15:23.520190 Iter 33400: Training Loss = 5063.6392\n",
      "2018-01-31 21:19:00.691594 Iter 33500: Training Loss = 664.1563\n",
      "2018-01-31 21:22:38.158804 Iter 33600: Training Loss = 560.7872\n",
      "2018-01-31 21:26:15.351347 Iter 33700: Training Loss = 54.8762\n",
      "2018-01-31 21:29:52.574671 Iter 33800: Training Loss = 547.2121\n",
      "2018-01-31 21:33:29.734089 Iter 33900: Training Loss = 3190.0454\n",
      "2018-01-31 21:37:06.895791 Iter 34000: Training Loss = 569.1975\n",
      "2018-01-31 21:40:44.152846 Iter 34100: Training Loss = 1085.9901\n",
      "2018-01-31 21:44:21.289402 Iter 34200: Training Loss = 427.4573\n",
      "2018-01-31 21:47:58.277365 Iter 34300: Training Loss = 38.9239\n",
      "2018-01-31 21:51:35.615757 Iter 34400: Training Loss = 667.0673\n",
      "2018-01-31 21:55:12.838284 Iter 34500: Training Loss = 132.4533\n",
      "2018-01-31 21:58:50.112059 Iter 34600: Training Loss = 720.8074\n",
      "2018-01-31 22:02:27.153969 Iter 34700: Training Loss = 908.6773\n",
      "2018-01-31 22:06:04.471443 Iter 34800: Training Loss = 79.1517\n",
      "2018-01-31 22:09:41.818857 Iter 34900: Training Loss = 532.2700\n",
      "2018-01-31 22:13:19.017481 Iter 35000: Training Loss = 1596.8577\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-35000\n",
      "2018-01-31 22:16:59.151725 Iter 35100: Training Loss = 202.2002\n",
      "2018-01-31 22:20:36.308717 Iter 35200: Training Loss = 557.2665\n",
      "2018-01-31 22:24:13.461230 Iter 35300: Training Loss = 880.1589\n",
      "2018-01-31 22:27:50.538485 Iter 35400: Training Loss = 4436.7681\n",
      "2018-01-31 22:31:27.834020 Iter 35500: Training Loss = 824.7742\n",
      "2018-01-31 22:35:04.765267 Iter 35600: Training Loss = 149.8018\n",
      "2018-01-31 22:38:41.667810 Iter 35700: Training Loss = 1096.8312\n",
      "2018-01-31 22:42:18.829285 Iter 35800: Training Loss = 623.5312\n",
      "2018-01-31 22:45:55.877027 Iter 35900: Training Loss = 578.6673\n",
      "2018-01-31 22:49:33.175021 Iter 36000: Training Loss = 609.1692\n",
      "2018-01-31 22:53:10.249019 Iter 36100: Training Loss = 1112.5103\n",
      "2018-01-31 22:56:47.367620 Iter 36200: Training Loss = 325.1137\n",
      "2018-01-31 23:00:24.272945 Iter 36300: Training Loss = 1883.1898\n",
      "2018-01-31 23:04:01.523326 Iter 36400: Training Loss = 271.2582\n",
      "2018-01-31 23:07:38.792058 Iter 36500: Training Loss = 10.7103\n",
      "2018-01-31 23:11:15.516595 Iter 36600: Training Loss = 903.3702\n",
      "2018-01-31 23:14:52.539527 Iter 36700: Training Loss = 2179.0947\n",
      "2018-01-31 23:18:29.627963 Iter 36800: Training Loss = 939.0354\n",
      "2018-01-31 23:22:06.617820 Iter 36900: Training Loss = 1160.8611\n",
      "2018-01-31 23:25:43.562022 Iter 37000: Training Loss = 671.1158\n",
      "2018-01-31 23:29:20.494508 Iter 37100: Training Loss = 1296.5626\n",
      "2018-01-31 23:32:57.739119 Iter 37200: Training Loss = 726.8425\n",
      "2018-01-31 23:36:34.777178 Iter 37300: Training Loss = 1570.5825\n",
      "2018-01-31 23:40:11.843757 Iter 37400: Training Loss = 1302.6525\n",
      "2018-01-31 23:43:48.741341 Iter 37500: Training Loss = 1103.2610\n",
      "2018-01-31 23:47:25.884426 Iter 37600: Training Loss = 712.2990\n",
      "2018-01-31 23:51:02.698371 Iter 37700: Training Loss = 942.6414\n",
      "2018-01-31 23:54:39.624088 Iter 37800: Training Loss = 1168.6219\n",
      "2018-01-31 23:58:16.570460 Iter 37900: Training Loss = 2072.2180\n",
      "2018-02-01 00:01:53.467498 Iter 38000: Training Loss = 808.3594\n",
      "2018-02-01 00:05:30.577006 Iter 38100: Training Loss = 493.8444\n",
      "2018-02-01 00:09:07.663342 Iter 38200: Training Loss = 2200.7520\n",
      "2018-02-01 00:12:44.801229 Iter 38300: Training Loss = 10.5064\n",
      "2018-02-01 00:16:21.780642 Iter 38400: Training Loss = 653.2068\n",
      "2018-02-01 00:19:58.575274 Iter 38500: Training Loss = 343.6543\n",
      "2018-02-01 00:23:35.364468 Iter 38600: Training Loss = 751.8191\n",
      "2018-02-01 00:27:12.187383 Iter 38700: Training Loss = 176.7027\n",
      "2018-02-01 00:30:49.279655 Iter 38800: Training Loss = 365.0705\n",
      "2018-02-01 00:34:26.155896 Iter 38900: Training Loss = 408.2141\n",
      "2018-02-01 00:38:03.026026 Iter 39000: Training Loss = 544.2520\n",
      "2018-02-01 00:41:40.142329 Iter 39100: Training Loss = 98.3953\n",
      "2018-02-01 00:45:17.163821 Iter 39200: Training Loss = 998.8566\n",
      "2018-02-01 00:48:54.026422 Iter 39300: Training Loss = 500.7688\n",
      "2018-02-01 00:52:30.811718 Iter 39400: Training Loss = 543.5906\n",
      "2018-02-01 00:56:07.591264 Iter 39500: Training Loss = 323.0080\n",
      "2018-02-01 00:59:44.415278 Iter 39600: Training Loss = 252.0547\n",
      "2018-02-01 01:03:21.348632 Iter 39700: Training Loss = 888.6025\n",
      "2018-02-01 01:06:58.094806 Iter 39800: Training Loss = 332.3459\n",
      "2018-02-01 01:10:34.781128 Iter 39900: Training Loss = 2109.7661\n",
      "2018-02-01 01:14:11.680064 Iter 40000: Training Loss = 222.5340\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-40000\n",
      "2018-02-01 01:17:51.711638 Iter 40100: Training Loss = 613.8043\n",
      "2018-02-01 01:21:28.684578 Iter 40200: Training Loss = 386.1166\n",
      "2018-02-01 01:25:05.365942 Iter 40300: Training Loss = 78.0890\n",
      "2018-02-01 01:28:42.026483 Iter 40400: Training Loss = 983.9314\n",
      "2018-02-01 01:32:19.030708 Iter 40500: Training Loss = 390.5142\n",
      "2018-02-01 01:35:55.946772 Iter 40600: Training Loss = 1027.7634\n",
      "2018-02-01 01:39:32.731384 Iter 40700: Training Loss = 1839.6194\n",
      "2018-02-01 01:43:09.479610 Iter 40800: Training Loss = 458.6812\n",
      "2018-02-01 01:46:46.479178 Iter 40900: Training Loss = 395.5958\n",
      "2018-02-01 01:50:23.528498 Iter 41000: Training Loss = 0.9284\n",
      "2018-02-01 01:54:00.711707 Iter 41100: Training Loss = 89.8718\n",
      "2018-02-01 01:57:37.389278 Iter 41200: Training Loss = 86.8978\n",
      "2018-02-01 02:01:14.506148 Iter 41300: Training Loss = 142.6171\n",
      "2018-02-01 02:04:51.239261 Iter 41400: Training Loss = 1023.5847\n",
      "2018-02-01 02:08:28.092472 Iter 41500: Training Loss = 648.9841\n",
      "2018-02-01 02:12:04.996524 Iter 41600: Training Loss = 1701.9211\n",
      "2018-02-01 02:15:42.149731 Iter 41700: Training Loss = 2519.2585\n",
      "2018-02-01 02:19:19.054669 Iter 41800: Training Loss = 416.2284\n",
      "2018-02-01 02:22:55.802876 Iter 41900: Training Loss = 164.2046\n",
      "2018-02-01 02:26:32.607872 Iter 42000: Training Loss = 1584.3754\n",
      "2018-02-01 02:30:09.393366 Iter 42100: Training Loss = 619.8696\n",
      "2018-02-01 02:33:46.212914 Iter 42200: Training Loss = 1210.0945\n",
      "2018-02-01 02:37:23.277881 Iter 42300: Training Loss = 2333.3201\n",
      "2018-02-01 02:41:00.104570 Iter 42400: Training Loss = 1555.6447\n",
      "2018-02-01 02:44:36.862821 Iter 42500: Training Loss = 1641.5621\n",
      "2018-02-01 02:48:13.866026 Iter 42600: Training Loss = 487.9276\n",
      "2018-02-01 02:51:50.684815 Iter 42700: Training Loss = 1297.6652\n",
      "2018-02-01 02:55:27.597572 Iter 42800: Training Loss = 1276.5032\n",
      "2018-02-01 02:59:04.728305 Iter 42900: Training Loss = 62.3045\n",
      "2018-02-01 03:02:41.511203 Iter 43000: Training Loss = 520.5122\n",
      "2018-02-01 03:06:18.361035 Iter 43100: Training Loss = 378.1183\n",
      "2018-02-01 03:09:55.145304 Iter 43200: Training Loss = 236.8662\n",
      "2018-02-01 03:13:32.013958 Iter 43300: Training Loss = 207.0594\n",
      "2018-02-01 03:17:08.930269 Iter 43400: Training Loss = 173.8972\n",
      "2018-02-01 03:20:45.953894 Iter 43500: Training Loss = 907.5375\n",
      "2018-02-01 03:24:22.579436 Iter 43600: Training Loss = 404.2460\n",
      "2018-02-01 03:27:59.522121 Iter 43700: Training Loss = 89.8667\n",
      "2018-02-01 03:31:36.271587 Iter 43800: Training Loss = 457.2813\n",
      "2018-02-01 03:35:13.110470 Iter 43900: Training Loss = 1154.0529\n",
      "2018-02-01 03:38:49.766531 Iter 44000: Training Loss = 1211.2135\n",
      "2018-02-01 03:42:26.629108 Iter 44100: Training Loss = 615.0393\n",
      "2018-02-01 03:46:03.536267 Iter 44200: Training Loss = 772.1748\n",
      "2018-02-01 03:49:40.376278 Iter 44300: Training Loss = 217.9006\n",
      "2018-02-01 03:53:17.205046 Iter 44400: Training Loss = 1950.5808\n",
      "2018-02-01 03:56:54.232770 Iter 44500: Training Loss = 1150.1110\n",
      "2018-02-01 04:00:30.993078 Iter 44600: Training Loss = 162.4539\n",
      "2018-02-01 04:04:07.851389 Iter 44700: Training Loss = 1445.4218\n",
      "2018-02-01 04:07:44.724546 Iter 44800: Training Loss = 274.6449\n",
      "2018-02-01 04:11:21.519676 Iter 44900: Training Loss = 618.6849\n",
      "2018-02-01 04:14:58.328829 Iter 45000: Training Loss = 1310.3944\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-45000\n",
      "2018-02-01 04:18:38.143355 Iter 45100: Training Loss = 0.4078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-01 04:22:14.823729 Iter 45200: Training Loss = 443.9996\n",
      "2018-02-01 04:25:51.797347 Iter 45300: Training Loss = 1268.5164\n",
      "2018-02-01 04:29:28.434882 Iter 45400: Training Loss = 1945.3839\n",
      "2018-02-01 04:33:05.111364 Iter 45500: Training Loss = 594.7296\n",
      "2018-02-01 04:36:41.936460 Iter 45600: Training Loss = 1580.1082\n",
      "2018-02-01 04:40:18.711780 Iter 45700: Training Loss = 820.3978\n",
      "2018-02-01 04:43:55.364036 Iter 45800: Training Loss = 433.5038\n",
      "2018-02-01 04:47:32.120917 Iter 45900: Training Loss = 531.2731\n",
      "2018-02-01 04:51:08.986146 Iter 46000: Training Loss = 678.2197\n",
      "2018-02-01 04:54:45.743155 Iter 46100: Training Loss = 810.8578\n",
      "2018-02-01 04:58:22.345168 Iter 46200: Training Loss = 1273.8517\n",
      "2018-02-01 05:01:59.025731 Iter 46300: Training Loss = 480.9303\n",
      "2018-02-01 05:05:35.914415 Iter 46400: Training Loss = 346.1341\n",
      "2018-02-01 05:09:12.619658 Iter 46500: Training Loss = 234.9200\n",
      "2018-02-01 05:12:49.445928 Iter 46600: Training Loss = 403.1935\n",
      "2018-02-01 05:16:26.241092 Iter 46700: Training Loss = 928.6404\n",
      "2018-02-01 05:20:02.966921 Iter 46800: Training Loss = 587.3430\n",
      "2018-02-01 05:23:39.790202 Iter 46900: Training Loss = 92.7744\n",
      "2018-02-01 05:27:16.426290 Iter 47000: Training Loss = 1307.4932\n",
      "2018-02-01 05:30:53.164721 Iter 47100: Training Loss = 1211.2903\n",
      "2018-02-01 05:34:30.008690 Iter 47200: Training Loss = 971.0896\n",
      "2018-02-01 05:38:06.808423 Iter 47300: Training Loss = 480.2419\n",
      "2018-02-01 05:41:43.540229 Iter 47400: Training Loss = 660.9816\n",
      "2018-02-01 05:45:20.265766 Iter 47500: Training Loss = 191.1923\n",
      "2018-02-01 05:48:57.037809 Iter 47600: Training Loss = 185.3970\n",
      "2018-02-01 05:52:33.898910 Iter 47700: Training Loss = 193.9779\n",
      "2018-02-01 05:56:10.660839 Iter 47800: Training Loss = 137.2244\n",
      "2018-02-01 05:59:47.311480 Iter 47900: Training Loss = 1013.2465\n",
      "2018-02-01 06:03:24.129153 Iter 48000: Training Loss = 807.2139\n",
      "2018-02-01 06:07:00.936646 Iter 48100: Training Loss = 515.7908\n",
      "2018-02-01 06:10:37.629432 Iter 48200: Training Loss = 105.5499\n",
      "2018-02-01 06:14:14.604180 Iter 48300: Training Loss = 161.2971\n",
      "2018-02-01 06:17:51.464520 Iter 48400: Training Loss = 112.9350\n",
      "2018-02-01 06:21:28.327296 Iter 48500: Training Loss = 473.1790\n",
      "2018-02-01 06:25:05.071740 Iter 48600: Training Loss = 843.4791\n",
      "2018-02-01 06:28:41.961650 Iter 48700: Training Loss = 840.7062\n",
      "2018-02-01 06:32:18.547552 Iter 48800: Training Loss = 481.0541\n",
      "2018-02-01 06:35:55.448779 Iter 48900: Training Loss = 765.9601\n",
      "2018-02-01 06:39:32.248677 Iter 49000: Training Loss = 271.2943\n",
      "2018-02-01 06:43:09.176682 Iter 49100: Training Loss = 929.0787\n",
      "2018-02-01 06:46:45.724694 Iter 49200: Training Loss = 2524.5898\n",
      "2018-02-01 06:50:22.501635 Iter 49300: Training Loss = 167.1536\n",
      "2018-02-01 06:53:59.226612 Iter 49400: Training Loss = 43.8344\n",
      "2018-02-01 06:57:36.040509 Iter 49500: Training Loss = 2180.3306\n",
      "2018-02-01 07:01:12.754741 Iter 49600: Training Loss = 326.3950\n",
      "2018-02-01 07:04:49.533838 Iter 49700: Training Loss = 1534.3065\n",
      "2018-02-01 07:08:26.285939 Iter 49800: Training Loss = 353.6203\n",
      "2018-02-01 07:12:02.999255 Iter 49900: Training Loss = 916.6917\n",
      "2018-02-01 07:15:39.846388 Iter 50000: Training Loss = 530.8607\n",
      "INFO:tensorflow:models\\astream_parent\\astream_parent.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Model saved in file: models\\astream_parent\\astream_parent.ckpt-50000\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "# Train the network without side outputs supervision\n",
    "with tf.Graph().as_default():\n",
    "    with tf.device('/gpu:' + str(gpu_id)):\n",
    "        global_step = tf.Variable(max_training_iters_2, name='global_step', trainable=False)\n",
    "        learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "        model.train_parent(dataset, imagenet_ckpt, 3, learning_rate, logs_path, max_training_iters_3, save_step,\n",
    "                           display_step, global_step, segnet_stream, iter_mean_grad=iter_mean_grad, resume_training=True,\n",
    "                           test_image_path=test_image, ckpt_name=ckpt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training losses & learning rate\n",
    "You should see training curves similar to the following:\n",
    "![](img/astream_parent_dsn2_loss.png)\n",
    "\n",
    "![](img/astream_parent_dsn3_loss.png)\n",
    "\n",
    "![](img/astream_parent_dsn4_loss.png)\n",
    "\n",
    "![](img/astream_parent_dsn5_loss.png)\n",
    "\n",
    "![](img/astream_parent_main_loss.png)\n",
    "\n",
    "![](img/astream_parent_total_loss.png)\n",
    "\n",
    "![](img/astream_parent_learning_rate.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
